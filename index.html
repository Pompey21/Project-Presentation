<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1400" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/il-capo.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">Vision Is All You Need: <br>A Vision-Only Approach to Dynamics
              Estimation in Autonomous Navigation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="mailto:lazar.milikic@epfl.ch" target="_blank">Lazar Milikic</a>,</span>
              <span class="author-block">
                <a href="mailto:ahmad.khan@epfl.ch" target="_blank">Ahmad Jarrar</a>,</span>
              <span class="author-block">
                <a href="mailto:said.gurbuz@epfl.ch" target="_blank">Said Gurbuz</a>,</span>
              <span class="author-block">
                <a href="mailto:marko.mekjavic@epfl.ch" target="_blank">Marko Mekjavic</a>,</span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Ecole Polytechnique Federale de Lausanne (EPFL)</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <!-- Progress Proposal PDF link -->
                <span class="link-block">
                  <a href="static/pdfs/Project_Proposal.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Project Proposal</span>
                  </a>
                </span>


                <!-- Supplementary PDF link -->
                <!-- Progress Report PDF link -->
                <span class="link-block">
                  <a href="static/pdfs/Project_Progress_Report.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Progress Report</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/Ahmad-Jarrar/cs503-vi-project" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!--                <span class="link-block">-->
                <!--                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"-->
                <!--                  class="external-link button is-normal is-rounded is-dark">-->
                <!--                  <span class="icon">-->
                <!--                    <i class="ai ai-arxiv"></i>-->
                <!--                  </span>-->
                <!--                  <span>arXiv</span>-->
                <!--                </a>-->
                <!--              </span>-->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <!-- Your video here -->
          <source src="static/videos/snippet.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          A snippet of a working run - we can observe the drone navigating through the desired gates without a problem.
          This example shows the drone navigating in an unbothered environment, where there are no projectiles that
          could potentially interupt this task.
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">

    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Inspired by biological systems' proficiency in navigating complex
              environments using primarily visual
              inputs, we propose a novel, vision-only approach to enable an autonomous drone to navigate highly dynamic
              environments effectively. Unlike traditional methods that use complex sensor arrays to capture
              environmental
              dynamics, our project aimed at developing a simpler, cost-effective visual-based model. Specifically, the
              project investigates whether a drone can solely rely on visual information to estimate the dynamics of
              objects and make optimal decisions in dynamic settings. We employ a two-phase training process, initially
              leveraging both visual observations and privileged information about environment dynamics to teach the
              drone to navigate through gates while evading projectiles. The subsequent phase involves refining a
              Dynamics Module to predict environmental dynamics from visual inputs alone. This approach could
              revolutionize the design and deployment of autonomous systems, reducing reliance on expensive sensors
              and increasing accessibility across various applications.
            </p>
          </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">

        <h2 class="title is-3">Overview</h2>
        <div class="tile is-ancestor is-centered" style="vertical-align: middle;">
          <div class="tile is-parent">
            <a href="#intro" class="tile is-child box">
              <p class="subtitle">
                <font size="-1">Introduction</font>
              </p>
            </a>
          </div>

          <div class="tile is-parent">
            <a href="#related-work" class="tile is-child box">
              <p class="subtitle">
                <font size="-1">Related Works</font>
              </p>
            </a>
          </div>

          <div class="tile is-parent">
            <a href="#methodology" class="tile is-child box">
              <p class="subtitle">
                <font size="-1">Methodology</font>
              </p>
            </a>
          </div>

          <div class="tile is-parent">
            <a href="#experiments" class="tile is-child box">
              <p class="subtitle">
                <font size="-1">Experiments</font>
              </p>
            </a>
          </div>

          <div class="tile is-parent">
            <a href="#conclusion" class="tile is-child box">
              <p class="subtitle">
                <font size="-1">Conclusion and Limitations</font>
              </p>
            </a>
          </div>

          <div class="tile is-parent">
            <a href="#contributions" class="tile is-child box">
              <p class="subtitle">
                <font size="-1">Individual Contributions</font>
              </p>
            </a>
          </div>
        </div>

      </div>
    </div>


  </section>
  <!-- End paper abstract -->


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Introduction. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3" id="intro">I. Introduction</h2>
          <div class="content has-text-justified">
            <p>
              Biological systems excel at navigating complex environments using primarily visual inputs <a
                href="https://arxiv.org/abs/2103.04672">[1]</a>.
              Studies show that human pilots control drones using video streams from the vehicle’s onboard camera
              without explicit
              state estimation or trajectory planning. This effectiveness is due to their ability to select
              task-relevant visual
              information <a href="https://www.sciencedirect.com/science/article/pii/S0301008296000603">[2]</a>
              <a href="https://arxiv.org/abs/2210.14985">[3]</a>. Inspired by this, our project challenges the
              conventional multi-sensor
              approach in autonomous systems, proposing that visual inputs alone can suffice for effective navigation in
              dynamic
              environments. Unlike traditional methods relying on complex sensors to grasp environmental dynamics
              <a href=https://www.science.org/doi/10.1126/scirobotics.abh1221>[4]</a> <a
                href=https://arxiv.org/abs/1801.05086>[5]</a>
              <a href=https://dl.acm.org/doi/10.1145/3301273>[6]</a>, our project aims to develop a vision-based model
              for an autonomous drone, thus
              reducing system complexity and cost, and increasing flexibility and accessibility for various
              applications.
            </p>
            <!-- <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 2em;"> -->
            <p>
              The core problem addressed in this project is whether an autonomous agent can rely solely on visual
              information to estimate the dynamics
              of objects within its environment and make optimal decisions in dynamic settings. To test the agent's
              ability to capture dynamics, we designed
              a specific task and environment: an autonomous drone must navigate through a series of gates (checkpoints)
              while evading incoming projectiles,
              relying entirely on visual input to estimate their dynamics.
            </p>
            <!-- Add the gif video of drone falling here -->
            <!-- Displaying the GIF using an img tag -->
            <img src="static/videos/DroneFalling.gif" alt="Drone Falling" id="tree"
              style="display: block; margin: 0 auto; width: 60%; height: auto;">
            <!-- </div> -->



            <p>
              This task mirrors real-world situations where countries explore drone applications for defense, law
              enforcement, and search and rescue operations
              <a href="https://www.airmedandrescue.com/latest/long-read/combat-search-and-rescue-drone">[]</a>
              <a
                href="https://www.karveinternational.com/insights/the-global-impact-of-ukraines-drone-revolution-on-military-forces">[]</a>.
              Despite recent advancements in vision-based autonomous drones
              <a href="https://arxiv.org/abs/2309.09865">[]</a>
              <a href="https://arxiv.org/abs/2210.14985">[3]</a>,
              a critical gap remains in effectively capturing environmental dynamics, especially in complex scenarios.
              The agent's success depends on (i)
              learning to extract meaningful visual representations of environmental dynamics and (ii) training a
              high-performance policy leveraging these cues.
            </p>

            <p>
              Our methodology utilizes contrastive learning to train a perception network to extract meaningful feature
              representations from complex,
              high-dimensional images. This training enables the network to concentrate on relevant visual features
              while ignoring irrelevant elements,
              as discussed in <a href="https://arxiv.org/abs/2210.14985">[3]</a>.
            </p>
            <p>
              Subsequently, we develop a vision-based control policy using a privileged learning approach. Initially, we
              train a state-based policy using
              reinforcement learning (RL) to maximize the drone's performance. This training utilizes privileged
              information concerning the dynamics and
              relative positions of projectiles and checkpoints around the drone <a
                href=https://arxiv.org/abs/2103.08624>[]</a>. Then, we examine the impact of privileged information
              on policy learning and how policies based on this information can be leveraged to create vision-based
              policies that infer environmental dynamics
              from visual cues.
            </p>
            <p>
              We propose two approaches: (i) imitation learning to distill knowledge into a vision-based policy without
              relying on privileged state information
              <a href="https://arxiv.org/abs/2210.14985">[3]</a> and (ii) training a dynamics module to predict
              privileged information directly from visual and state observations, using
              the optimized privileged-based policy, similar to the RMA framework <a
                href=https://arxiv.org/abs/2107.04034>[]</a>.
            </p>
            <p>
              Our experiments in a <i>PyBullet Drone</i> simulator <a href=https://arxiv.org/abs/2103.02142>[]</a> show
              promising results. Both approaches demonstrate the ability
              to understand and react to projectiles while moving toward checkpoints. However, a performance gap between
              privileged-based and vision-based policies
              remains, indicating ample space for further research and improvement.
            </p>
          </div>
        </div>
      </div>
      <!--/ Introduction. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Related work. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3" id="related-work">II. Related works</h2>
          <div class="content has-text-justified">
            <p>
              Despite the scarcity of public research specifically focused on drones avoiding or
              interacting with projectiles, numerous studies address various aspects of drone navigation.
              A prominent area of interest is drone racing, where drones navigate through a series of gates,
              a topic extensively explored in recent works
              <a href="https://arxiv.org/abs/2210.14985">[3]</a>
              <a href="http://dx.doi.org/10.1126/scirobotics.abh1221">[4]</a>
              [9]
              [13].
              We build upon these studies by incorporating projectiles, thereby increasing the complexity
              and rigorously testing the drone's ability to interpret environmental dynamics.
            </p>
            <p>
              Early research on autonomous drone navigation and racing primarily utilized state-based
              methods that depend on precise global positioning data, using this information to train neural
              networks for reinforcement learning (RL) policies
              ~\cite{pham2018autonomous, song2020flightmare}.
              <a href="https://arxiv.org/abs/2210.14985">[5]</a> [13]
              For example, Song et al.
              [13]
              trained an RL agent to race through gates using
              their global position and orientation. However, our approach differs in that we use relative positions
              based on camera data, moving towards an approximation of these values through vision, thus reducing
              reliance on privileged information—a limitation not addressed by traditional state-based methods.
            </p>
            <p>
              Vision-based methods in drone racing have initially focused on leveraging direct trajectory planning
              through visual inputs. Foehn et al.
              <a href="http://dx.doi.org/10.1126/scirobotics.abh1221">[4]</a>
              implemented a system that combines visual-inertial
              odometry with CNN-based detection of gate corners for robust state estimation. Subsequently, a trajectory
              planner would generate optimal paths using motion primitives derived from a point-mass model of the drone.
              Nevertheless, this assumption does not accurately represent the drone's true actuation limits and may
              result
              in dynamically unfeasible trajectories. In contrast, our method is inspired by Fu et al.
              <a href="https://arxiv.org/abs/2210.14985">[3]</a> ,
              who trained a vision-based policy using imitation learning. The policy learns from a teacher model with
              privileged information (gate positions), thus implicitly extracting relevant details from visual cues.
              Our project aims to extend this methodology to more complex scenarios, testing the robustness of the model
              in settings where gates are randomly placed yet visible from the previous gate, challenging the student
              policy
              to avoid overfitting to the teacher's optimized routes.
            </p>
            <!-- slika iz prezentacije -->
            <p>
              Our approach to the Dynamics Module methodology is heavily influenced by the Rapid Model Adaptation (RMA)
              framework
              [11],
              which pioneers an adaptation module with a two-phase training procedure.
              Although RMA can integrate visual inputs
              <a href=" https://api.semanticscholar.org/CorpusID: 244896056">[14]</a>,
              its application does not maximize the
              potential of vision in dynamic settings due to task constraints. We have therefore restructured their
              approach
              to focus specifically on predicting environmental dynamics and managing a vision-based system, which we
              believe
              can better harness the capabilities of vision, especially for our specific task. We retain the
              foundational
              two-phase training concept but modify the architecture to capture the dynamics, aligning the system's
              inductive
              bias with our project's needs.
            </p>
          </div>
        </div>
      </div>
      <!--/ Related work. -->
    </div>
  </section>



  <section class="section">
    <div class="container is-max-desktop">
      <!-- Methodology. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3" id="methodology">III. Methodology</h2>
          <div class="content has-text-justified">

            <h3 class="title is-4">III-A. Task Formulation</h3>
            <div class="content has-text-justified">
              <p>
                Our task involves navigating a drone through a series of gates while dodging incoming projectiles, 
                framing this challenge as a dual-objective optimization problem. The primary objectives are to maximize 
                the number of gates navigated and minimize the time taken to transit between these gates. The drone's 
                hardware setup includes a single forward-facing camera and an inertial measurement unit (IMU) that tracks 
                the drone's orientation, velocity, and acceleration
              </p>
              <p>
                In our dynamic environment, gates are randomly generated at various heights and orientations, aligned 
                along the \(x\)-axis. Additionally, the environment launches projectiles randomly from variable positions, 
                targeting the drone based on its current trajectory. These projectiles are fired with velocities calculated 
                to intersect with the drone's path, considering both the drone's motion and position, as well as, gravitational 
                effects at the moment of launch.
              </p>
              <p>
                For the simplicity of modeling projectile dynamics, we define a random variable \(t\) representing the time from the 
                projectile's launch to its potential impact on the drone. The firing velocity of the projectile $\mathbf{v}^p$ is 
                then determined as follows:
                \begin{align}
                \begin{cases}
                    v^p_{x} = \frac{d_{x}}{t} + v^d_{x},\\
                    v^p_{y} = \frac{d_{y}}{t} + v^d_{y},\\
                    v^p_{z} = \frac{d_{z}}{t} + v^d_{z} + \frac{1}{2}gt^2,
                \end{cases}\label{eq:proj}
                \end{align}
                where \(d\) denotes the distance vector from the projectile launch point to the drone at the time of firing, \(v^d\) 
                represents the drone’s velocity, and \(g\) is the gravitational acceleration. This environmental setup compels the drone 
                agent to deeply understand and interpret the dynamics and layout of its surroundings based purely on its observational 
                inputs, to enable effective navigation through the defined tasks.
              </p>
            </div>

            <h3 class="title is-4">III-B. Base-privileged Policy</h3>
            <div class="content has-text-justified">
              <p>
                Initially, we establish a base-privileged policy \(\pi_{\text{privileged}}\) that acts as a benchmark 
                for evaluating other models and for foundational training. This policy is theoretically designed to optimize 
                the drone's performance, maximally, in our specified tasks, and it also serves as a starting point for 
                developing vision-based solutions.
              </p>
              
              <h3 class="title is-5">III-B-1. Observation Space</h3>
              <div class="content has-text-justified">
                <p>
                  The drone's observation space includes various metrics from the inertial measurement unit (IMU), such as velocity 
                  \(\mathbf{v}^d = (v^d_x, v^d_y, v^d_z)\), angular velocity \(\mathbf{w}^d = (w^d_x, w^d_y, w^d_z)\), and orientation 
                  parameters—roll, pitch, and yaw \(\mathbf{r} = (r, p, y)\). These metrics facilitate faster and more stable learning. 
                  In the privileged setting, the drone receives data on the relative position and orientation of the next gate \(\mathbf{g} = 
                  (g_x, g_y, g_z, \phi_x, \phi_y, \phi_z)\) as viewed from the drone’s camera. Additionally, the privileged information includes 
                  the relative positions and distance \(\mathbf{p}_i = (p_{i_x}, p_{i_y}, p_{i_z}, d)\) and velocities of projectiles \(\mathbf{v}^p_i 
                  = (v^p_{i_x}, v^p_{i_y}, v^p_{i_z})\) for each \(i\)-th airborne projectile attacking the drone, where \(i \in [1, \dots, P_{\max}]\), 
                  with \(P_{\max}\) denoting the maximum number of projectiles that can be airborne simultaneously. These cumulative projectile data points 
                  \(\mathbf{p}\) and \(\mathbf{v}^p\) ensure the model accounts for dynamic threats. If fewer than \(P_{\max}\) projectiles are present, 
                  non-existent projectiles are represented by zero-padding.
                </p>
              </div>

              <h3 class="title is-5">III-B-2. Policy Network</h3>
              <div class="content has-text-justified">
                <p>
                  For the base-privileged policy \(\pi_{\text{privileged}}\), we use a multilayer perceptron (MLP) as the neural network architecture 
                  for the policy head. Similar to RMA~\cite{kumar2021rma}, the policy uses the current drone state (including the previously taken action) 
                  $\textbf{s}_t = [\mathbf{v}^d_t,\, \mathbf{w}^d_t,\, \mathbf{r}_t,\, \boldsymbol{a}_{t-1}] \in \mathbb{R}^{13}$ and encoded privileged 
                  information, called the extrinsic vector, $\mathbf{z}_t \in \mathbb{R}^8$, to decide the next action. We obtain the extrinsic vector 
                  $\mathbf{z}_t$ by passing it through an MLP, denoted as $\mu$, such that $\mu(\mathbf{g}_t,\, \mathbf{p}_t,\, \mathbf{v^p}_t) = \mathbf{z}_t$.
                </p>
              </div>

              <h3 class="title is-5">III-B-3. Action Space</h3>
              <div class="content has-text-justified">
                <p>
                  The action space of the drone includes controlling the thrust levels of its four rotors. Specifically, the base-privileged policy maps the 
                  observational input to the thrust levels of these rotors, which are then converted to RPMs for each rotor at every time step \(t\), expressed 
                  as \(\pi_{\text{privileged}}(\textbf{s}_t, \mathbf{z}_t) = \boldsymbol{a}_t\), where \(\boldsymbol{a}_t = (w^1_t, w^2_t, w^3_t, w^4_t)\).
                </p>
              </div>

              <h3 class="title is-5">III-B-4. Reward Function</h3>
              <div class="content has-text-justified">
                <p>
                  We use Reinforcement Learning (RL) to train the policy $\pi_\text{privileged}$ of our base-privileged agent to 
                  maximize the reward provided by the environment. Given the complexity of the tasks—passing through gates and evading 
                  projectiles—a straightforward reward function proved insufficient for mastering multiple tasks simultaneously. 
                  Therefore, we segmented the learning process into three sequential curriculum phases:

                  <h4 class="title is-5.5">1] Basic Stabilization and Hovering:</h4>
                  <div class="content has-text-justified">
                    <p>
                      The first phase focuses on basic stabilization and learning to hover toward gates. High rewards are given for maintaining stability, 
                      and penalties are imposed for crashing or excessive tilting. The reward function at each step $t$ is:

                      \begin{align}
                      \begin{split}
                          R(\mathbf{s}_t, \boldsymbol{a}_t, \mathbf{s}_{t+1}) &= t - R_{\text{crashed}} \times \mathbb{1}_{\text{crashed}}\\
                          &- |\max(r, p, y)| \times \mathbb{1}_{\text{tilted}}\\
                          &+ \lambda_\text{dist} \left(\|{\mathbf{s}_t - \mathbf{g}_t}\| - \|{\mathbf{s}_{t+1} - \mathbf{g}_t}\right\|),
                      \end{split}
                      \end{align}

                      where $\mathbb{1}_{\text{crashed}}$ indicates if the drone crashed, incurring a high penalty, and $\mathbb{1}_{\text{tilted}}$ 
                      indicates if the drone is excessively tilted, with penalties based on the largest tilting angle. A progress reward is given based 
                      on the distance change to the next gate, modulated by $\lambda_\text{dist}$.
                    </p>
                  </div>


                  <h4 class="title is-5.5">2] Navigation:</h4>
                  <div class="content has-text-justified">
                    <p>
                      The second phase emphasizes navigation. Rewards increase as the drone decreases the distance to gates and successfully 
                      navigates through them. The reward function at each time step $t$ is:

                      \begin{align}
                      \begin{split}
                          R(\mathbf{s}_t, \boldsymbol{a}_t, \mathbf{s}_{t+1}) &= R_{\text{passed}} \times \mathbb{1}_{\text{passed}}\\
                          &- R_{\text{crashed}} \times \mathbb{1}_{\text{crashed}}\\
                          &+ \lambda_\text{dist} \left(\|{\mathbf{s}_t - \mathbf{g}_t}\| - \|{\mathbf{s}_{t+1} - \mathbf{g}_t}\right\|),
                      \end{split}
                      \end{align}

                      where $\mathbb{1}_{\text{passed}}$ indicates when a gate is successfully navigated, earning a high reward, and 
                      $\mathbb{1}_{\text{crashed}}$ indicates a crash, incurring a high penalty. A progress reward is given for approaching 
                      the next gate, modulated by $\lambda_\text{dist}$.
                    </p>
                  </div>


                  <h4 class="title is-5.5">3] Evasion and Navigation:</h4>
                  <div class="content has-text-justified">
                    <p>
                      In the final phase, the drone must navigate gates and evade projectiles. We introduce projectiles without 
                      penalizing the drone for being hit initially, as previous trials led to a "suicide" strategy to avoid penalties. 
                      Instead, we increase the penalty for crashes and add a minor reward for survival. Only one projectile is active at 
                      a time. The reward function is:

                      \begin{align}
                      \begin{split}
                          R(\mathbf{s}_t, \boldsymbol{a}_t, \mathbf{s}_{t+1}) &= R_{\text{passed}} \times \mathbb{1}_{\text{passed}}\\
                          &- R_{\text{crashed}} \times \mathbb{1}_{\text{crashed}}\\
                          &+ \lambda_\text{dist} \left(\|{\mathbf{s}_t - \mathbf{g}_t}\| - \|{\mathbf{s}_{t+1} - \mathbf{g}_t}\right\|)\\
                          &+ R_{\text{projectile}} \times \#~\text{airborne projectiles},
                      \end{split}
                      \end{align}


                      where $\mathbb{1}_{\text{passed}}$ indicates when a gate is successfully navigated, earning a high reward, and 
                      $\mathbb{1}_{\text{crashed}}$ indicates a crash, incurring a high penalty. A progress reward is given for approaching 
                      the next gate, modulated by $\lambda_\text{dist}$, and an additional reward $R_{\text{projectile}}$ is given for each 
                      airborne projectile.
                    </p>
                  </div>
                </p>
              </div>



            </div>

            <h3 class="title is-4">III-C. Image Feature Representation Learning</h3>
            <div class="content has-text-justified">
              <p>
                To transition from a base-privileged policy to a vision-based policy, we need an effective image 
                feature extractor that can produce meaningful visual features. Effective navigation through dynamic 
                environments using visual inputs necessitates extracting latent representations that capture relevant 
                information about projectiles, targets, and environmental dynamics. We employ a two-stage approach to 
                learn visual feature embeddings from raw image observations.
              </p>
              <p>
                In the first stage, we fine-tune the YOLOv5 model~\cite{yolov5}, so we can leverage its powerful object 
                detection capabilities. Specifically, we captured 2,500 images from our simulated environment and annotated 
                the bounding boxes for the current gate, next gate, and projectiles. We then fine-tuned a pre-trained 
                YOLOv5s model with this custom dataset. This step allows the model to accurately detect and localize 
                these critical entities within the visual scene, effectively converting the object detector model into 
                a customized feature extractor.
              </p>
              <p>
                The second stage involves extracting compact, yet informative visual features from the fine-tuned YOLOv5s 
                model's output. We append an average pooling layer to the model's detection head, which downsamples the 
                output feature maps. The resulting low-dimensional feature vectors are then concatenated, and L2 
                normalization is applied for stability during training.
              </p>
            </div>

            <h3 class="title is-4">IV-D. Student Imitation Policy</h3>
            <div class="content has-text-justified">
              <p>
                Once we have obtained a well-performing teacher policy that can navigate the drone through gates while 
                avoiding projectiles and an image feature extractor that produces meaningful visual features, we can 
                distill the base-privileged policy knowledge into a vision-based student policy. The key difference is 
                that the student policy relies exclusively on the last $k$ time steps of drone states $\textbf{s}_{t-k:t}$ 
                and visual observations $\textbf{o}_{t-k:t}$ without direct access to privileged information. The drone 
                state $\mathbf{s}_t$ is given as in Section~\ref{eq:state}.
              </p>
              <p>
                The student policy architecture comprises three main components: a feature extractor, a memory-based network, 
                and a policy network. The feature extractor, as described in the previous section, converts the raw image 
                observations $\mathbf{o}_{t-k:t}$ into compact visual embeddings $\text{YOLOv5}(\mathbf{o}_{t-k:t}) = 
                \mathbf{z}_{t-k:t}\in\mathbb{R}^{T\times 128}$. These embeddings, concatenated with the drone state information, 
                serve as the input to the memory-based network.
              </p>
              <p>
                We created two alternative architectures for the memory-based network: Temporal Convolutional Network (TCN) 
                and Transformer-based Network.
              </p>
              <p>
                For the first architecture, we employ a Temporal Convolutional Network (TCN)~\cite{lea2016temporal} to capture 
                temporal dependencies and extract relevant information from the history of observations and states. The TCN 
                operates on the sequence of concatenated embeddings $[\mathbf{s}_t \oplus \mathbf{z}_t,\, \mathbf{s}_{t-1} \oplus 
                \mathbf{z}_{t-1}, \dots,\, \mathbf{s}_{t-k} \oplus \mathbf{z}_{t-k}]$, where~$\oplus$ denotes concatenation, 
                producing a rich temporal representation that encodes the conditions of the environment. We call this model StudentTCN.
              </p>
              <p>
                For the second architecture, inspired by the success of transformer architectures in various sequence modeling tasks, 
                including applying Transformers for policy modeling~\cite{chen2021decision}, we explored a transformer-based network. 
                Similar to the TCN, this network receives a concatenated sequence of drone states and visual embeddings as input. 
                Instead of using temporal convolutions, we employed a transformer encoder module~\cite{vaswani2023attention} to capture 
                long-range dependencies and intricate interactions between state and visual information across time steps. We call this 
                model StudentTransformer.
              </p>
              <p>
                Finally, a multi-layer perceptron (MLP) policy network takes the memory-based network's output, aggregated using mean pooling, 
                as input and predicts the desired control command, mirroring the action space of the teacher policy. Through this imitation 
                learning process, the student policy learns to implicitly infer relevant gate information from visual cues, mimicking the 
                teacher's optimal decision-making without relying on privileged state information. Formally, the student policy can be expressed as:
                  $$\pi_\text{studentTCN}(\textbf{s}_{t-k:t}, \textbf{o}_{t-k:t}) = \text{MLP}(\text{TCN}([\mathbf{s}_t \oplus \mathbf{z}_t,\,  \mathbf{s}_{t-1} \oplus \mathbf{z}_{t-1}, \dots]))$$
                  $$\pi_\text{studentTrans}(\textbf{s}_{t-k:t}, \textbf{o}_{t-k:t}) = \text{MLP}(\text{Trans}([\mathbf{s}_t \oplus \mathbf{z}_t,\,  \mathbf{s}_{t-1} \oplus \mathbf{z}_{t-1}, \dots]))$$
              </p>
              <p>
                The optimization objective for imitation learning is defined as the mean squared error between the outputs of the teacher policy and the student policy:
                \begin{align}
                    \mathcal{L}(\theta) = \|{\pi_\text{student}(\textbf{s}_{t-k:t}, \textbf{o}_{t-k:t}|\theta) - \pi_\text{privileged}(\textbf{s}_t, \mathbf{z}_t)}\|^2_2
                \end{align}
                where $\theta$ represents the learnable parameters of the student policy.
              </p>
            </div>

            <h3 class="title is-4">V-E. Drone Dynamics Module-based Policy (DDMP)</h3>
            <div class="content has-text-justified">
              <p>
                Building upon insights from the Rapid Model Adaptation
                 (RMA) [11] framework, we propose an alternative
                approach for leveraging visual observations for navigation
                in dynamic environments. Instead of distilling knowledge
                from a privileged-based policy, we train a dedicated Drone
                Dynamics Module to estimate environmental dynamics from
                visuals and state information.
              </p>
              <p>
                The Drone Dynamics Module (DDM) is a transformer-based architecture that receives two inputs: 
                a sequence of the last $k$ drone state vectors $\textbf{s}_{t-k:t}$ and a sequence of the last 
                $k$ visual embeddings $\mathbf{z}_{t-k:t}$ extracted from the corresponding images $\mathbf{o}_{t-k:t}$. 
                Specifically, we use the compact visual embeddings obtained from the fine-tuned YOLOv5 feature extractor, 
                $\text{YOLOv5}(\mathbf{o}_{t-k:t}) = \mathbf{z}_{t-k:t} \in \mathbb{R}^{k \times 128}$, as described in 
                the Image Feature Representation Learning section.
              </p>
              <p>
                Formally, DDM operates on the sequence of concatenated embeddings $[\mathbf{s}_t \oplus \mathbf{z}_t,\, 
                \mathbf{s}_{t-1} \oplus \mathbf{z}_{t-1}, \dots,\, \mathbf{s}_{t-k} \oplus \mathbf{z}_{t-k}]$. 
                This sequence is processed through transformer encoder layers, capturing intricate temporal dependencies 
                and cross-modal interactions between the drone's motion and the visual observations.
              </p>
              <p>
                The transformer encoder's output is then aggregated using mean pooling and passed through an MLP to predict 
                encoded privileged dynamics information, called the extrinsic vector $\mathbf{\hat{z}}_t$ as described in 
                Section~\ref{eq:state}.
              </p>
              <p>
                Given the trained and optimized privileged-based policy, we optimize the DDM to predict the extrinsic vector 
                of privileged information using the mean squared error loss:
                \begin{align}
                    \mathbf{\hat{z}} = \text{DDM}([\mathbf{s}_t \oplus\mathbf{z}_t,&\, \mathbf{s}_{t-1} \oplus \mathbf{z}_{t-1}, \dots,\, \mathbf{s}_{t-k} \oplus \mathbf{z}_{t-k}]|\omega)\\
                  &\mathcal{L}(\omega) = \|{\mathbf{\hat{z}}_t - \mathbf{z}_t }\|^2_2
                \end{align}
                where $\omega$ represents the learnable parameters of the DDM.
              </p>
              <p>
                The DDMP policy is then defined as:
                \begin{align}
                    \pi_\text{DDMP}\left(\mathbf{s}_t, \text{DDM}(\textbf{s}_{t-k:t}, \textbf{o}_{t-k:t})\right) = \pi_{\text{privileged}}(\textbf{s}_t, \mathbf{z}_t)
                \end{align}
              </p>
            </div>

          </div>
        </div>
      </div>
      <!--/ Methodology. -->
    </div>
  </section>





  <!-- Image carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/carousel1.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              First image description.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/carousel2.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Second image description.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/carousel3.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Third image description.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/carousel4.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Fourth image description.
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End image carousel -->



  


<section class="section">
  <div class="container is-max-desktop">
    <!-- Experiments. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="experiments">IV. Experiments</h2>
        <div class="content has-text-justified">
          <h3 class="title is-4">Experimental Setup</h3>
          <div>
            <h4 class="title is-5.5"> 1] Simulator Environment</h4>
            <div class="content has-text-justified">
              <p>
                We utilize the Gym Pybullet Drones environment~\cite{panerati2021learning} for its simplicity and realistic physics simulations. 
                While alternatives like Flightmare~\cite{song2020flightmare} and AirSim~\cite{airsim2017fsr} offer more detailed realism, their 
                complexity and the time required for mastery make them less suitable for our project's timeline. PyBullet is chosen for its 
                straightforward integration with the Gymnasium library, ease of customization in Python, and sufficient realism.
              </p>
              <p>
                Given the camera's limited field of view, the environment is only partially observable, posing challenges in spatial awareness 
                and reacting to dynamic obstacles. To address this, we implement several adjustments to create a theoretical framework for successful 
                vision-based navigation. These modifications optimize the drone's performance and adaptability when relying primarily on visual data.
              </p>
              <p>
                We set the gates' orientation and position on the $y$-axis such that the maximum angle between consecutive gates is $22.5^\circ$ to 
                facilitate manageable turns and visibility of successive gates. Both training and evaluation involve passing through five gates in total. 
                During training, the gates are randomly placed at the beginning of each episode. For evaluation, the drone's performance is assessed in 
                six environments with randomly positioned gates, each with fixed and different seeds.
              </p>
              <p>
                Projectiles are fired towards the drone with a probability of $0.02$ per time step, ensuring the drone's camera can capture them. 
                This is to accommodate the simulation's single-camera limitation, meaning projectiles are fired in the direction the camera is facing 
                at the moment of firing. A maximum of two projectiles may be airborne simultaneously to maintain balanced challenge levels.
              </p>
            </div>

            <h4 class="title is-5.5"> 2] Evaluation</h4>
            <div class="content has-text-justified">
              <p>
                To evaluate our policies, we designed several metrics. We measure the average number of gates passed before the drone crashes due to a 
                projectile hit or any other reason. Additionally, we measure the success rate, defined as the percentage of episodes where the drone 
                completes all five gates. We also measure the average time taken to transit between two consecutive gates. These metrics capture the 
                essential aspects of the optimization problem: passing through as many checkpoints as possible as quickly as possible. Finally, to 
                provide more context for comparisons, we measure the average number of projectiles encountered per episode before either completing the 
                episode or being struck down by a projectile.
              </p>
              <p>
                These metrics are calculated across 20 gate layouts with different seeds, where each layout is evaluated for 30 seconds, approximately 
                corresponding to 3 episodes.
              </p>
            </div>
          </div>

          <h3 class="title is-4">Training Details</h3>
          <div class="title is-6">

            <h4 class="title is-5.5"">1] Base-privileged Policy Training</h4>
            <div class="content has-text-justified" style="font-weight: normal;">
              <p>
                The MLP $\mu$ that maps privileged information to the extrinsic vector $\mathbf{z}_t$ is a 3-layer MLP with hidden layer 
                sizes of 256 and 128. It encodes a privileged vector $[\mathbf{g}_t,\, \mathbf{p}_t,\, \mathbf{v^p}_t] \in \mathbb{R}^{20}$ 
                (6-dimensional vector for gate orientation, and two 7-dimensional vectors for up to two airborne projectiles) into an extrinsic 
                vector $\mathbf{z}_t \in \mathbb{R}^8$. The base policy head is a 4-layer MLP with hidden sizes of 128, 64, 32, and 16. It takes as 
                input the current drone state $\textbf{s}_t = [\mathbf{v}^d_t,\, \mathbf{w}^d_t,\, \mathbf{r}_t,\, \boldsymbol{a}_{t-1}] \in \mathbb{R}^{13}$ 
                and the extrinsic vector $\mathbf{z}_t \in \mathbb{R}^8$, and outputs the next action $\boldsymbol{a}_t$. GELU activation~\cite{gelu} is used 
                for all MLP layers. This choice was made despite \textit{Stable Baselines3}'s recommendation for Tanh activation~\cite{stable-baselines}, 
                as Tanh led to slower learning and convergence of the agent even at the initial curriculum step.
              </p>
              <p>
                We trained the policy using Proximal Policy Optimization (PPO)~\cite{schulman2017proximal} from \textit{Stable Baselines3}~\cite{stable-baselines}, 
                using its default training parameters and conducting rollouts in 32 parallel environments. Although Soft Actor-Critic (SAC)~\cite{haarnoja2018soft} 
                is often recommended as a good or even better alternative for continuous control problems, it resulted in much slower convergence during the initial 
                curriculum step for stabilization and hovering.
              </p>
              <p>
                The reward constants from Section~\ref{sec:reward} used to train base-privileged agents are as follows: $R_\text{crashed}=5$ for the first two 
                curriculum steps, and $R_\text{crashed}=50$ in the third step to impose a higher penalty for drone crashes caused by projectiles, enabling faster 
                learning. The reward for passing gates is $R_\text{passed}=100$. The distance progress reward is scaled with $\lambda_\text{dist}=10$ to amplify the 
                usually small distance progress rewards, and the reward for encountering a projectile is $R_\text{projectile}=2$.
              </p>
              <p>
                A small final note regarding the base-privileged agent: Following the proposals from Song et al.~\cite{song2021autonomous} and Fu et al.~\cite{fu2022learning}, 
                we provided the agent with privileged positional and orientation information for the next gate and the one after it. While this approach enabled the policy to 
                perform well during the first two curriculum steps, the agent did not successfully learn to avoid projectiles.
              </p>
            </div>

            <h4 class="title is-5.5"> 2] Student Policy and DDMP Training</h4>
            <div class="content has-text-justified" style="font-weight: normal;">
              <p>
                As described in Section~\ref{sec:student}, for the student policy, we explore both TCN and Transformer architectures for memory-based networks. The TCN uses 
                6 layers of convolutions with a hidden size of 128. The Transformer uses 6 layers of Transformer encoder layers, also with a hidden size of 128. The policy 
                head for both architectures is a 4-layer MLP with hidden sizes of 128, 64, 32, and 16, similar to the policy head for base-privileged agents.
              </p>
              <p>
                For the DDMP, we experiment only with the Transformer architecture for the Drone Dynamics Module due to the discouraging results obtained with TCN, as discussed 
                in Section~\ref{sec:results}. This setup also uses 6 layers of Transformer encoder layers with a hidden size of 128.
              </p>
              <p>
                For both vision-based implementations, we use the previous drone states and observations from the last 0.5 seconds, corresponding to $k=12$ previous steps. This 
                helps the model estimate the dynamics of the environment, either implicitly (student policy) or explicitly (DDMP).
              </p>
              <p>
                To collect the training data of the state-action history for these policies, we unroll the trained base policy $\pi_\text{privileged}$~\cite{fu2022learning}.
              </p>
              <p>
                For training the student policy through imitation learning and training the DDM, we use the AdamW optimizer~\cite{loshchilov2019decoupled} with a learning 
                rate of $1e-4$, training for 300 epochs. The MultiStepLR scheduler decreases the learning rate by a factor of 0.5 every 100 epochs.
              </p>
            </div>

            <h4 class="title is-5.5"> 3] Training Hardware</h4>
            <div class="content has-text-justified" style="font-weight: normal;">
              <p>
                The models were trained using a CUDA-enabled NVIDIA V100 PCIe 32 GB GPU with 7 TFLOPS provided by the Scitas cluster.
              </p>
            </div>

          </div>

          <h3 class="title is-4">Policy Comparison</h3>
          <div class="content has-text-justified">
            <p>
              The results show a significant discrepancy between the baseline privileged agent and our two approaches. We also observe that the complex 
              dynamics in this setup are not adequately captured by less descriptive TCN models, necessitating the use of Transformer architecture for 
              both the student policy and drone dynamics module. Direct comparisons indicate that the DDMP approach slightly outperforms the student 
              policy approach in both metrics.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Experiments. -->
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Conclusion & Limitations. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="conclusion">V. Conclusion & Limitations</h2>
        <div class="content has-text-justified">
        

        </div>
      </div>
    </div>
    <!--/ Conclusion & Limitations. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Individual Contributions. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="contributions">VI. Individual Contributions</h2>
        <div class="content has-text-justified">
        

        </div>
      </div>
    </div>
    <!--/ Individual Contributions. -->
  </div>
</section>





  <!-- Youtube video -->
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <!-- Paper video. -->
        <h2 class="title is-3">Video Presentation</h2>
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">

            <div class="publication-video">
              <!-- Youtube embed code here -->
              <iframe src="https://youtu.be/FgNuYghDCjg" frameborder="0" allow="autoplay; encrypted-media"
                allowfullscreen></iframe>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End youtube video -->


  <!-- Video carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Another Carousel</h2>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-video1">
            <video poster="" id="video1" autoplay controls muted loop height="100%">
              <!-- Your video file here -->
              <source src="static/videos/carousel1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-video2">
            <video poster="" id="video2" autoplay controls muted loop height="100%">
              <!-- Your video file here -->
              <source src="static/videos/carousel2.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-video3">
            <video poster="" id="video3" autoplay controls muted loop height="100%">\
              <!-- Your video file here -->
              <source src="static/videos/carousel3.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End video carousel -->






  <!-- Paper poster -->
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">Poster</h2>

        <iframe src="static/pdfs/sample.pdf" width="100%" height="550">
        </iframe>

      </div>
    </div>
  </section>
  <!--End paper poster -->


  <!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section> -->
  <!--End BibTex citation -->
  <section class="section">
    <div class="container is-max-desktop">

      <!-- Concurrent Work. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">References</h2>

          <div class="content has-text-justified">
            <style>
              .references {
                list-style-type: none;
                /* Removes default numbering */
                counter-reset: ref-counter;
                /* Create a new instance of counter */
              }

              .references li {
                counter-increment: ref-counter;
                /* Increment the counter */
                margin-bottom: 5px;
                /* Optional: adds spacing between items */
              }

              .references li::before {
                content: "[" counter(ref-counter) "] ";
                /* Format the counter */
                font-weight: bold;
                /* Optional: makes the number bold */
              }
            </style>
            <ol class="references">
              <li>
                O. TRULLIER, S. I. WIENER, A. BERTHOZ, and J.-A. MEYER, “Biologically based artificial navigation
                systems: Review and prospects,” Progress in Neurobiology, vol. 51, no. 5, pp. 483–544, 1997. [Online].
                Available: <a
                  href="www.sciencedirect.com/science/article/pii/S0301008296000603">sciencedirect:S0301008296000603</a>.
              </li>
              <li>
                C. Pfeiffer and D. Scaramuzza, “Human-piloted drone racing: Visual processing and control,” IEEE
                Robotics and Automation Letters, vol. 6, no. 2, pp. 3467–3474, 2021.
              </li>
              <li>
                J. Fu, Y. Song, Y. Wu, F. Yu, and D. Scaramuzza, “Learn- ing deep sensorimotor policies for vision-based
                autonomous drone racing,” 2022.
              </li>
              <li>
                P. Foehn, A. Romero, and D. Scaramuzza, “Time- optimal planning for quadrotor waypoint flight,” Science
                Robotics, vol. 6, no. 56, Jul. 2021. [Online]. Available: <a
                  href="http://dx.doi.org/10.1126/scirobotics.abh1221">scirobotics.abh1221</a>
              </li>
              <li>
                H. X. Pham, H. M. La, D. Feil-Seifer, and L. V. Nguyen, “Autonomous uav navigation using reinforcement
                learning,” 2018.
              </li>
              <li>
                W. Koch, R. Mancuso, R. West, and A. Bestavros, “Reinforcement learning for uav attitude control,” ACM
                Trans. Cyber-Phys. Syst., vol. 3, no. 2, feb 2019. [Online]. Available: <a
                  href="https://doi.org/10.1145/3301273">doi.3301273</a>
              </li>
              <li>
                C. J. A. A. (Retired), “Combat search and rescue by drone,” Aug 2023. [Online]. Available: <a
                  href="www.airmedandrescue.com/latest/long-read/combat-search-and-rescue-drone">combat-search-and-rescue-drone</a>
              </li>
              <li>
                C. Chell, “The global impact of ukraine’s drone revolution on military forces,” Mar 2024. [Online].
                Available: <a
                  href="https://www.karveinternational.com/insights/the-global-impact-of-ukraines-drone-revolution-on-military-forces">global-impact-of-ukraines-drone-revolution</a>
              </li>
              <li>
                J. Xing, L. Bauersfeld, Y. Song, C. Xing, and D. Scaramuzza, “Contrastive learning for enhancing robust
                scene transfer in vision-based agile flight,” 2024.
              </li>
              <li>
                Y. Song, M. Steinweg, E. Kaufmann, and D. Scaramuzza, “Autonomous drone racing with deep reinforcement
                learning,” 2021.
              </li>
              <li>
                A.Kumar,Z.Fu,D.Pathak,andJ.Malik,“Rma:Rapidmotor adaptation for legged robots,” 2021.
              </li>
              <li>
                J. Panerati, H. Zheng, S. Zhou, J. Xu, A. Prorok, and A. P. Schoellig, “Learning to fly – a gym
                environment with pybullet physics for reinforcement learning of multi-agent quadcopter control,” 2021.
              </li>
              <li>
                Y. Song, S. Naji, E. Kaufmann, A. Loquercio, and D. Scara- muzza, “Flightmare: A flexible quadrotor
                simulator,” in Proceedings of the 2020 Conference on Robot Learning, 2021, pp. 1147–1157.
              </li>
              <li>
                Z. Fu, A. Kumar, A. Agarwal, H. Qi, J. Malik, and D. Pathak, “Coupling vision and proprioception for
                navigation of legged robots,” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition
                (CVPR), pp. 17252–17262, 2021. [Online]. Available: <a
                  href="https://api.semanticscholar.org/CorpusID:244896056">CorpusID:244896056</a>
              </li>
              <li>
                Ultralytics, “ultralytics/yolov5: v7.0 - YOLOv5 SOTA Realtime Instance Segmentation,”
                https://github.com/ultralytics/yolov5, 2022, accessed: 1st July, 2024. [Online]. Available: <a
                  href="https://doi.org/10.5281/zenodo.7347926">zenodo.7347926</a>
              </li>
              <li>
                C. Lea, R. Vidal, A. Reiter, and G. D. Hager, “Temporal convolutional networks: A unified approach to
                action segmentation,” 2016.
              </li>
              <li>
                L.Chen,K.Lu,A.Rajeswaran,K.Lee,A.Grover,M.Laskin, P. Abbeel, A. Srinivas, and I. Mordatch, “Decision
                trans- former: Reinforcement learning via sequence modeling,” 2021.
              </li>
              <li>
                A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin,
                “Attention is all you need,” 2023.
              </li>
              <li>
                S. Shah, D. Dey, C. Lovett, and A. Kapoor, “Airsim: High-fidelity visual and physical simulation for
                autonomous vehicles,” in Field and Service Robotics, 2017. [Online]. Available: <a
                  href="https://arxiv.org/abs/1705.05065">arxiv.1705.05065</a>
              </li>

              <!-- Additional list items as previously defined -->
            </ol>
          </div>
        </div>
      </div>
      <!--/ Concurrent Work. -->

    </div>
  </section>



  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the of this website, we just ask that you link back to this page in the footer.
              <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

<script type="text/javascript" id="MathJax-script" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.0.0/es5/latest?tex-mml-chtml.js">
  </script>

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</html>