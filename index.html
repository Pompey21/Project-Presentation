<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1400"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/il-capo.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">Vision Is All You Need: <br>A Vision-Only Approach to Dynamics Estimation in Autonomous Navigation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="mailto:lazar.milikic@epfl.ch" target="_blank">Lazar Milikic</a>,</span>
                <span class="author-block">
                  <a href="mailto:ahmad.khan@epfl.ch" target="_blank">Ahmad Jarrar</a>,</span>
                  <span class="author-block">
                    <a href="mailto:said.gurbuz@epfl.ch" target="_blank">Said Gurbuz</a>,</span>
                    <span class="author-block">
                      <a href="mailto:marko.mekjavic@epfl.ch" target="_blank">Marko Mekjavic</a>,</span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Ecole Polytechnique Federale de Lausanne (EPFL)</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                      <!-- Supplementary PDF link -->
                    <!-- Progress Proposal PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/Project_Proposal.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Project Proposal</span>
                    </a>
                  </span>


                    <!-- Supplementary PDF link -->
                    <!-- Progress Report PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/Project_Progress_Report.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Progress Report</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Ahmad-Jarrar/cs503-vi-project" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
<!--                <span class="link-block">-->
<!--                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"-->
<!--                  class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                    <i class="ai ai-arxiv"></i>-->
<!--                  </span>-->
<!--                  <span>arXiv</span>-->
<!--                </a>-->
<!--              </span>-->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/snippet.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        A snippet of a working run - we can observe the drone navigating through the desired gates without a problem.
        This example shows the drone navigating in an unbothered environment, where there are no projectiles that
        could potentially interupt this task.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">

  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Inspired by biological systems' proficiency in navigating complex environments using primarily visual
            inputs, we propose a novel, vision-only approach to enable an autonomous drone to navigate highly dynamic
            environments effectively. Unlike traditional methods that use complex sensor arrays to capture environmental
            dynamics, our project aimed at developing a simpler, cost-effective visual-based model. Specifically, the
            project investigates whether a drone can solely rely on visual information to estimate the dynamics of
            objects and make optimal decisions in dynamic settings. We employ a two-phase training process, initially
            leveraging both visual observations and privileged information about environment dynamics to teach the
            drone to navigate through gates while evading projectiles. The subsequent phase involves refining a
            Dynamics Module to predict environmental dynamics from visual inputs alone. This approach could
            revolutionize the design and deployment of autonomous systems, reducing reliance on expensive sensors
            and increasing accessibility across various applications.
          </p>
        </div>
      </div>
    </div>
  </div>

  <div class="columns is-centered has-text-centered">
    <div class="column is-full-width">

      <h2 class="title is-3">Overview</h2>
      <div class="tile is-ancestor is-centered" style="vertical-align: middle;">
        <div class="tile is-parent">
          <a href="#intro" class="tile is-child box">
            <p class="subtitle"><font size="-1">Introduction</font></p>
          </a>
        </div>

        <div class="tile is-parent">
          <a href="#related-work" class="tile is-child box">
            <p class="subtitle"><font size="-1">Related Works</font></p>
          </a>
        </div>

        <div class="tile is-parent">
          <a href="#methodology" class="tile is-child box">
            <p class="subtitle"><font size="-1">Methodology</font></p>
          </a>
        </div>

        <div class="tile is-parent">
          <a href="#exp" class="tile is-child box">
            <p class="subtitle"><font size="-1">Experiments</font></p>
          </a>
        </div>

        <div class="tile is-parent">
          <a href="#conclusion" class="tile is-child box">
            <p class="subtitle"><font size="-1">Conclusion and Limitations</font></p>
          </a>
        </div>
      </div>

    </div>
  </div>


</section>
<!-- End paper abstract -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Introduction. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="intro">I. Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Biological systems excel at navigating complex environments using primarily visual inputs <a href="https://arxiv.org/abs/2103.04672">[1]</a>. 
            Studies show that human pilots control drones using video streams from the vehicle’s onboard camera without explicit 
            state estimation or trajectory planning. This effectiveness is due to their ability to select task-relevant visual 
            information <a href="https://www.sciencedirect.com/science/article/pii/S0301008296000603">[2]</a> 
            <a href="https://arxiv.org/abs/2210.14985">[3]</a>. Inspired by this, our project challenges the conventional multi-sensor 
            approach in autonomous systems, proposing that visual inputs alone can suffice for effective navigation in dynamic 
            environments. Unlike traditional methods relying on complex sensors to grasp environmental dynamics
            <a href=https://www.science.org/doi/10.1126/scirobotics.abh1221>[4]</a> <a href=https://arxiv.org/abs/1801.05086>[5]</a>
            <a href=https://dl.acm.org/doi/10.1145/3301273>[6]</a>, our project aims to develop a vision-based model for an autonomous drone, thus 
            reducing system complexity and cost, and increasing flexibility and accessibility for various applications.
          </p>
          <!-- <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 2em;"> -->
            <p>
              The core problem addressed in this project is whether an autonomous agent can rely solely on visual information to estimate the dynamics 
              of objects within its environment and make optimal decisions in dynamic settings. To test the agent's ability to capture dynamics, we designed 
              a specific task and environment: an autonomous drone must navigate through a series of gates (checkpoints) while evading incoming projectiles, 
              relying entirely on visual input to estimate their dynamics.
            </p>
            <!-- Add the gif video of drone falling here -->
             <!-- Displaying the GIF using an img tag -->
             <img src="static/videos/DroneFalling.gif" alt="Drone Falling" id="tree" style="display: block; margin: 0 auto; width: 60%; height: auto;">
             <!-- </div> -->
          
        

          <p>
            This task mirrors real-world situations where countries explore drone applications for defense, law enforcement, and search and rescue operations
            <a href="https://www.airmedandrescue.com/latest/long-read/combat-search-and-rescue-drone">[]</a>
            <a href="https://www.karveinternational.com/insights/the-global-impact-of-ukraines-drone-revolution-on-military-forces">[]</a>. 
            Despite recent advancements in vision-based autonomous drones
            <a href="https://arxiv.org/abs/2309.09865">[]</a>
            <a href="https://arxiv.org/abs/2210.14985">[3]</a>, 
            a critical gap remains in effectively capturing environmental dynamics, especially in complex scenarios. The agent's success depends on (i) 
            learning to extract meaningful visual representations of environmental dynamics and (ii) training a high-performance policy leveraging these cues.
          </p>

          <p>
            Our methodology utilizes contrastive learning to train a perception network to extract meaningful feature representations from complex, 
            high-dimensional images. This training enables the network to concentrate on relevant visual features while ignoring irrelevant elements, 
            as discussed in <a href="https://arxiv.org/abs/2210.14985">[3]</a>.
          </p>
          <p>
            Subsequently, we develop a vision-based control policy using a privileged learning approach. Initially, we train a state-based policy using 
            reinforcement learning (RL) to maximize the drone's performance. This training utilizes privileged information concerning the dynamics and 
            relative positions of projectiles and checkpoints around the drone <a href=https://arxiv.org/abs/2103.08624>[]</a>. Then, we examine the impact of privileged information 
            on policy learning and how policies based on this information can be leveraged to create vision-based policies that infer environmental dynamics 
            from visual cues.
          </p>
          <p>
            We propose two approaches: (i) imitation learning to distill knowledge into a vision-based policy without relying on privileged state information
            <a href="https://arxiv.org/abs/2210.14985">[3]</a> and (ii) training a dynamics module to predict privileged information directly from visual and state observations, using 
            the optimized privileged-based policy, similar to the RMA framework <a href=https://arxiv.org/abs/2107.04034>[]</a>.
          </p>
          <p>
            Our experiments in a <i>PyBullet Drone</i> simulator <a href=https://arxiv.org/abs/2103.02142>[]</a> show promising results. Both approaches demonstrate the ability 
            to understand and react to projectiles while moving toward checkpoints. However, a performance gap between privileged-based and vision-based policies 
            remains, indicating ample space for further research and improvement.
          </p>
        </div>
      </div>
    </div>
    <!--/ Introduction. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Related work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="related-work">II. Related works</h2>
        <div class="content has-text-justified">
          <p>
            Despite the scarcity of public research specifically focused on drones avoiding or 
            interacting with projectiles, numerous studies address various aspects of drone navigation. 
            A prominent area of interest is drone racing, where drones navigate through a series of gates, 
            a topic extensively explored in recent works
            <a href="https://arxiv.org/abs/2210.14985">[3]</a>
            <a href="http://dx.doi.org/10.1126/scirobotics.abh1221">[4]</a>
            [9]
            [13]. 
            We build upon these studies by incorporating projectiles, thereby increasing the complexity 
            and rigorously testing the drone's ability to interpret environmental dynamics.
          </p>
          <p>
            Early research on autonomous drone navigation and racing primarily utilized state-based 
            methods that depend on precise global positioning data, using this information to train neural 
            networks for reinforcement learning (RL) policies
            ~\cite{pham2018autonomous, song2020flightmare}.
            <a href="https://arxiv.org/abs/2210.14985">[5]</a> [13]
            For example, Song et al.
            [13]
            trained an RL agent to race through gates using 
            their global position and orientation. However, our approach differs in that we use relative positions 
            based on camera data, moving towards an approximation of these values through vision, thus reducing 
            reliance on privileged information—a limitation not addressed by traditional state-based methods.
          </p>
          <p>
            Vision-based methods in drone racing have initially focused on leveraging direct trajectory planning 
            through visual inputs. Foehn et al.
            <a href="http://dx.doi.org/10.1126/scirobotics.abh1221">[4]</a>
            implemented a system that combines visual-inertial 
            odometry with CNN-based detection of gate corners for robust state estimation. Subsequently, a trajectory 
            planner would generate optimal paths using motion primitives derived from a point-mass model of the drone. 
            Nevertheless, this assumption does not accurately represent the drone's true actuation limits and may result 
            in dynamically unfeasible trajectories. In contrast, our method is inspired by Fu et al.
            <a href="https://arxiv.org/abs/2210.14985">[3]</a> ,
            who trained a vision-based policy using imitation learning. The policy learns from a teacher model with 
            privileged information (gate positions), thus implicitly extracting relevant details from visual cues. 
            Our project aims to extend this methodology to more complex scenarios, testing the robustness of the model 
            in settings where gates are randomly placed yet visible from the previous gate, challenging the student policy 
            to avoid overfitting to the teacher's optimized routes.
          </p>
          <!-- slika iz prezentacije -->
          <p>
            Our approach to the Dynamics Module methodology is heavily influenced by the Rapid Model Adaptation (RMA) 
            framework
            [11], 
            which pioneers an adaptation module with a two-phase training procedure. 
            Although RMA can integrate visual inputs
            <a href=" https://api.semanticscholar.org/CorpusID: 244896056">[14]</a>, 
            its application does not maximize the 
            potential of vision in dynamic settings due to task constraints. We have therefore restructured their approach 
            to focus specifically on predicting environmental dynamics and managing a vision-based system, which we believe 
            can better harness the capabilities of vision, especially for our specific task. We retain the foundational 
            two-phase training concept but modify the architecture to capture the dynamics, aligning the system's inductive 
            bias with our project's needs. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Related work. -->
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Methodology. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="methodology">III. Methodology</h2>
        <div class="content has-text-justified">

          <h3 class="title is-4">III-A. Task Formulation</h3>
          <div class="content has-text-justified">
            <p>
              Our task involves navigating a drone through a series of gates while dodging incoming projectiles, framing this challenge 
              as a dual-objective optimization problem. The primary objectives are to maximize the number of gates navigated and minimize 
              the time taken to transit between these gates. The drone’s hardware setup includes a single forward-facing camera and an inertial 
              measurement unit (IMU) that tracks the drone’s orientation, velocity, and acceleration.
            </p>
            <p>
              In our dynamic environment, gates are randomly gener- ated at various heights and orientations, aligned along the x-axis. 
              Additionally, the environment launches projectiles randomly from variable positions, targeting the drone based on its current 
              trajectory. These projectiles are fired with velocities calculated to intersect with the drone’s path, considering both the drone’s 
              motion and position, as well as, gravitational effects at the moment of launch.
            </p>
            <p>
              or the simplicity of modeling projectile dynamics, we define a random variable t representing the time from the projectile’s launch 
              to its potential impact on the drone. The firing velocity of the projectile vp is then determined as follows:
            </p>
            <!-- here comes the equation -->
            <p>
              where d denotes the distance vector from the projectile launch point to the drone at the time of firing, vd represents the drone’s 
              velocity, and g is the gravitational acceleration. This environmental setup compels the drone agent to deeply understand and interpret 
              the dynamics and layout of its surroundings based purely on its observational inputs, to enable effective navigation through the defined tasks.
            </p>
          </div>

          <h3 class="title is-4">III-B. Base-privileged Policy</h3>
          <div class="content has-text-justified">
            <p>
              Initially, we establish a base-privileged policy πprivileged that acts as a benchmark for evaluating other models and for foundational 
              training. This policy is theoretically designed to optimize the drone’s performance, maximally, in our specified tasks, and it also serves 
              as a starting point for developing vision-based solutions.
            </p>
            <p>
              1) Observation Space: The drone’s observation space
              includes various metrics from the inertial measurement unit
              (IMU), such as velocity vd = (vxd, vyd, vzd), angular velocity wd = (wxd,wyd,wzd), and orientation parameters—roll,
              pitch, and yaw r = (r, p, y). These metrics facilitate faster
              and more stable learning. In the privileged setting, the
              drone receives data on the relative position of the next
              gate g = (gx , gy , gz ) as viewed from the drone’s camera.
              Additionally, the privileged information includes the relative
              positions pi = (pix , piy , piz ) and velocities of projectiles
              vp = (vp ,vp ,vp ) for each i-th airborne projectile at- i ix iy iz
              tacking the drone, where i ∈ [1, . . . , Pmax], with Pmax denoting the maximum number of projectiles that can be airborne simultaneously. 
              These cumulative projectile data points p and vp ensure the model accounts for dynamic threats. If fewer than Pmax projectiles are present, 
              nonexistent projectiles are represented by zero-padding.
            </p>
            <p>
              2) Policy Network: For the base-privileged policy πprivileged, we use a multilayer perceptron (MLP) as the neural network architecture for 
              the policy head. Similar to RMA [11], the policy uses the current drone state (including the previously taken action) st = [vtd , wtd , rt , at−1 ] 
              ∈ R13 and encoded privileged information, called the extrinsic vector, zt ∈ R8, to decide the next action. We obtain the extrinsic vector zt by 
              passing it through an MLP, denoted as μ, such that μ(gt, pt, vpt) = zt.
            </p>
            
            <p>
              where d denotes the distance vector from the projectile launch point to the drone at the time of firing, vd represents the drone’s 
              velocity, and g is the gravitational acceleration. This environmental setup compels the drone agent to deeply understand and interpret 
              the dynamics and layout of its surroundings based purely on its observational inputs, to enable effective navigation through the defined tasks.
            </p>
          </div>          

        </div>
      </div>
    </div>
    <!--/ Methodology. -->
  </div>
</section>


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->



<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="method">III. Methodology</h2>
        <div class="content has-text-justified">
          <p>
            In this section, we present the methodology utilized for this paper. We first discuss the environment setup,
            after which we detail how the predator and prey agents are created. Finally, we talk about how the agents are optimized.
          </p>
          <p>
            We create a 3D environment using the Unity game development engine <a href="https://unity.com/">[3]</a>. The selection of Unity as the engine
            for creating the environment instead of a different simulator is motivated by several key factors. Mainly, Unity's ML-Agents
            package <a href="https://arxiv.org/abs/1809.02627">[4]</a> offers support for the OpenAI Gym framework
            <a href="https://arxiv.org/abs/1809.02627">[5]</a>, which is widely used in
            reinforcement learning research. This way, Unity enables interaction between the environment and the learning algorithms,
            allowing for efficient experimentation and evaluation of predator-prey agents.
          </p>
          <p>
            Moreover, the engine offers a wide range of built-in tools, such as physics simulations, ray tracing, and collision.
            Finally, Unity provides a user-friendly and intuitive development environment, making it accessible to researchers with
             varying levels of expertise.
          </p>
        </div>
        <!-- Creating environments. -->
        <h3 class="title is-4">III-A. Environments</h3>
        <div class="content has-text-justified">
          <p>
            We present a diverse set of simulated environments aimed at replicating various scenarios involving predator-prey interactions.
            Our objective is to uncover a wide range of behaviors for the predators and prey. To achieve
            this diversity, we incorporate obstacles such as trees and rocks within the environments. Additionally, we introduce walls along
            the boundaries to confine the agents to the designated training area. This confinement allows us to concentrate the actions of an agent
             and observations within a controlled environment.
          </p>
          <p>
            Furthermore, we integrate physics-based simulations and collision detection mechanisms into the environment.
            By assigning colliders to the predator and prey agent models, we ensure accurate detection of interactions with the
            environment and other agents. Each object in the environment is assigned a corresponding tag, such as "obstacle",
            "predator", or "prey", enabling efficient identification.
          </p>
          <p>
            To speed up the learning process, we design the initial training environment ('Control' shown below) to be free of
            obstacles. This approach eliminates potential obstructions that could slow down training progress in the early stages.
            Subsequently, we create three additional training environments ('Forest', 'Escape Room' and 'Rocky') featuring distinct obstacle types:
            small trees, large rocks, and a split doorway. These environments allow the agents to acquire the skills necessary for
            navigating through increasingly complex situations involving obstacles, and should allow the prey agent to learn to exploit
             occlusions.
          </p>
        </div>
        <!--/ Creating environments. -->

      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Experiments. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="experiments">IV. Experiments</h2>
        <div class="content has-text-justified">
        </div>
      </div>
    </div>
    <!--/ Experiments. -->
  </div>
</section>



<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->






<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section> -->
<!--End BibTex citation -->
<section class="section">
  <div class="container is-max-desktop">

    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">References</h2>

        <div class="content has-text-justified">
          <style>
            .references {
              list-style-type: none; /* Removes default numbering */
              counter-reset: ref-counter; /* Create a new instance of counter */
            }
            .references li {
              counter-increment: ref-counter; /* Increment the counter */
              margin-bottom: 5px; /* Optional: adds spacing between items */
            }
            .references li::before {
              content: "[" counter(ref-counter) "] "; /* Format the counter */
              font-weight: bold; /* Optional: makes the number bold */
            }
          </style>
          <ol class="references">
            <li>
              O. TRULLIER, S. I. WIENER, A. BERTHOZ, and J.-A. MEYER, “Biologically based artificial navigation systems: Review and prospects,” Progress in Neurobiology, vol. 51, no. 5, pp. 483–544, 1997. [Online]. Available: <a href="www.sciencedirect.com/science/article/pii/S0301008296000603">sciencedirect:S0301008296000603</a>.
            </li>
            <li>
              C. Pfeiffer and D. Scaramuzza, “Human-piloted drone racing: Visual processing and control,” IEEE Robotics and Automation Letters, vol. 6, no. 2, pp. 3467–3474, 2021.
            </li>
            <li>
              J. Fu, Y. Song, Y. Wu, F. Yu, and D. Scaramuzza, “Learn- ing deep sensorimotor policies for vision-based autonomous drone racing,” 2022.
            </li>
            <li>
              P. Foehn, A. Romero, and D. Scaramuzza, “Time- optimal planning for quadrotor waypoint flight,” Science Robotics, vol. 6, no. 56, Jul. 2021. [Online]. Available: <a href="http://dx.doi.org/10.1126/scirobotics.abh1221">scirobotics.abh1221</a>
            </li>
            <li>
              H. X. Pham, H. M. La, D. Feil-Seifer, and L. V. Nguyen, “Autonomous uav navigation using reinforcement learning,” 2018.
            </li>
            <li>
              W. Koch, R. Mancuso, R. West, and A. Bestavros, “Reinforcement learning for uav attitude control,” ACM Trans. Cyber-Phys. Syst., vol. 3, no. 2, feb 2019. [Online]. Available: <a href="https://doi.org/10.1145/3301273">doi.3301273</a>
            </li>
            <li>
              C. J. A. A. (Retired), “Combat search and rescue by drone,” Aug 2023. [Online]. Available: <a href="www.airmedandrescue.com/latest/long-read/combat-search-and-rescue-drone">combat-search-and-rescue-drone</a>
            </li>
            <li>
              C. Chell, “The global impact of ukraine’s drone revolution on military forces,” Mar 2024. [Online]. Available: <a href="https://www.karveinternational.com/insights/the-global-impact-of-ukraines-drone-revolution-on-military-forces">global-impact-of-ukraines-drone-revolution</a>
            </li>
            <li>
              J. Xing, L. Bauersfeld, Y. Song, C. Xing, and D. Scaramuzza, “Contrastive learning for enhancing robust scene transfer in vision-based agile flight,” 2024.
            </li>
            <li>
              Y. Song, M. Steinweg, E. Kaufmann, and D. Scaramuzza, “Autonomous drone racing with deep reinforcement learning,” 2021.
            </li>
            <li>
              A.Kumar,Z.Fu,D.Pathak,andJ.Malik,“Rma:Rapidmotor adaptation for legged robots,” 2021.
            </li>
            <li>
              J. Panerati, H. Zheng, S. Zhou, J. Xu, A. Prorok, and A. P. Schoellig, “Learning to fly – a gym environment with pybullet physics for reinforcement learning of multi-agent quadcopter control,” 2021.
            </li>
            <li>
              Y. Song, S. Naji, E. Kaufmann, A. Loquercio, and D. Scara- muzza, “Flightmare: A flexible quadrotor simulator,” in Proceedings of the 2020 Conference on Robot Learning, 2021, pp. 1147–1157.
            </li>
            <li>
              Z. Fu, A. Kumar, A. Agarwal, H. Qi, J. Malik, and D. Pathak, “Coupling vision and proprioception for navigation of legged robots,” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 17252–17262, 2021. [Online]. Available: <a href="https://api.semanticscholar.org/CorpusID:244896056">CorpusID:244896056</a>
            </li>
            <li>
              Ultralytics, “ultralytics/yolov5: v7.0 - YOLOv5 SOTA Realtime Instance Segmentation,” https://github.com/ultralytics/yolov5, 2022, accessed: 1st July, 2024. [Online]. Available: <a href="https://doi.org/10.5281/zenodo.7347926">zenodo.7347926</a>
            </li>
            <li>
              C. Lea, R. Vidal, A. Reiter, and G. D. Hager, “Temporal convolutional networks: A unified approach to action segmentation,” 2016.
            </li>
            <li>
              L.Chen,K.Lu,A.Rajeswaran,K.Lee,A.Grover,M.Laskin, P. Abbeel, A. Srinivas, and I. Mordatch, “Decision trans- former: Reinforcement learning via sequence modeling,” 2021.
            </li>
            <li>
              A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” 2023.
            </li>
            <li>
              S. Shah, D. Dey, C. Lovett, and A. Kapoor, “Airsim: High-fidelity visual and physical simulation for autonomous vehicles,” in Field and Service Robotics, 2017. [Online]. Available: <a href="https://arxiv.org/abs/1705.05065">arxiv.1705.05065</a>
            </li>

            <!-- Additional list items as previously defined -->
          </ol>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>



  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
